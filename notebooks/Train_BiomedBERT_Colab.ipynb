{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üè• Medical Text Classification - BiomedBERT Training (Google Colab)\n",
    "\n",
    "**Purpose:** Train BiomedBERT model for medical text classification with GPU acceleration\n",
    "\n",
    "**Model:** `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext`\n",
    "\n",
    "**Expected Accuracy:** ~99%\n",
    "\n",
    "**Training Time:** ~10-15 minutes on Colab GPU\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Instructions:\n",
    "1. **Enable GPU:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
    "2. **Upload Dataset:** Upload `medical_texts.csv` to Colab files\n",
    "3. **Run All Cells:** Runtime ‚Üí Run all\n",
    "4. **Download Model:** After training, download the `biomedbert_model/` folder\n",
    "5. **Deploy:** Place the model folder in your project's `models/` directory\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Hyperparameters (Matching Original Notebook):\n",
    "- **Learning Rate:** 3e-5 (0.00003)\n",
    "- **Batch Size:** 16 (train), 8 (eval)\n",
    "- **Epochs:** 10\n",
    "- **Max Sequence Length:** 512\n",
    "- **Optimizer:** AdamW\n",
    "- **Train/Test Split:** 80/20 (stratified)\n",
    "- **Keyword Masking:** Enabled (removes disease names to prevent data leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## 2Ô∏è‚É£ Upload Dataset\n",
    "\n",
    "**Upload your `medical_texts.csv` file using the file upload button below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_file"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Please upload your medical_texts.csv file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify upload\n",
    "if 'medical_texts.csv' in uploaded:\n",
    "    print(\"‚úÖ File uploaded successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Error: medical_texts.csv not found. Please upload the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## 3Ô∏è‚É£ Load & Prepare Data (Exact Notebook Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_csv"
   },
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv('medical_texts.csv')\n",
    "print(f\"üìä Loaded {len(df)} records from CSV\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "filter_data"
   },
   "outputs": [],
   "source": [
    "# Define focus area mapping (EXACTLY from original notebook)\n",
    "focus_area_map = {\n",
    "    'Cancers': ['Breast Cancer', 'Prostate Cancer', 'Skin Cancer', 'Colorectal Cancer', 'Lung Cancer', 'Leukemia'],\n",
    "    'Cardiovascular Diseases': ['Stroke', 'Heart Failure', 'Heart Attack', 'High Blood Cholesterol', 'High Blood Pressure'],\n",
    "    'Metabolic & Endocrine Disorders': ['Causes of Diabetes', 'Diabetes', 'Diabetic Retinopathy', 'Hemochromatosis', 'Kidney Disease'],\n",
    "    'Neurological & Cognitive Disorders': [\"Alzheimer's Disease\", \"Parkinson's Disease\", 'Balance Problems'],\n",
    "    'Other Age-Related & Immune Disorders': ['Shingles', 'Osteoporosis', 'Age-related Macular Degeneration', 'Psoriasis', 'Gum (Periodontal) Disease', 'Dry Mouth']\n",
    "}\n",
    "\n",
    "# Create reverse mapping (condition -> focus_group)\n",
    "condition_to_focus_area = {\n",
    "    condition: focus_area\n",
    "    for focus_area, conditions in focus_area_map.items()\n",
    "    for condition in conditions\n",
    "}\n",
    "\n",
    "# Map focus_area to focus_group\n",
    "df['focus_group'] = df['focus_area'].map(condition_to_focus_area)\n",
    "\n",
    "# Drop rows without focus_group (not in top 25 focus areas)\n",
    "df = df.dropna(subset=['focus_group', 'answer'])\n",
    "\n",
    "print(f\"‚úÖ After filtering to 25 focus areas: {len(df)} records\")\n",
    "print(f\"\\nüìä Focus group distribution:\")\n",
    "print(df['focus_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deduplicate"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates based on answer column (like original notebook)\n",
    "df = df.drop_duplicates(subset='answer')\n",
    "print(f\"‚úÖ After deduplication: {len(df)} records\")\n",
    "print(f\"   Expected: ~628 records (matching original notebook)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "encode_labels"
   },
   "outputs": [],
   "source": [
    "# Encode focus groups to numeric labels (EXACTLY from original notebook)\n",
    "focus_map = {\n",
    "    'Neurological & Cognitive Disorders': 0,\n",
    "    'Cancers': 1,\n",
    "    'Cardiovascular Diseases': 2,\n",
    "    'Metabolic & Endocrine Disorders': 3,\n",
    "    'Other Age-Related & Immune Disorders': 4\n",
    "}\n",
    "\n",
    "df['label'] = df['focus_group'].map(focus_map)\n",
    "\n",
    "print(f\"‚úÖ Label encoding complete\")\n",
    "print(f\"\\nüìä Label distribution:\")\n",
    "print(df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_test_split"
   },
   "outputs": [],
   "source": [
    "# Extract texts and labels (use 'answer' column like original notebook)\n",
    "texts = df['answer'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Train/test split (80/20, stratified by label)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train/Test split complete\")\n",
    "print(f\"   Train samples: {len(train_texts)}\")\n",
    "print(f\"   Test samples: {len(test_texts)}\")\n",
    "print(f\"   Expected: ~484 train, ~121 test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocessing"
   },
   "source": [
    "## 4Ô∏è‚É£ Text Preprocessing - Keyword Masking (Critical Step!)\n",
    "\n",
    "**This removes disease keywords to prevent data leakage and force the model to learn from symptoms/descriptions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keyword_masking"
   },
   "outputs": [],
   "source": [
    "# Define keywords to remove (EXACTLY from original notebook)\n",
    "remove_keywords = [\n",
    "    'Breast Cancer', 'Prostate Cancer', 'Skin Cancer',\n",
    "    'Colorectal Cancer', 'Lung Cancer', 'Leukemia', 'Stroke', 'Heart Failure', 'Heart Attack',\n",
    "    'High Blood Cholesterol', 'High Blood Pressure', 'Causes of Diabetes', 'Diabetes', 'Diabetic Retinopathy',\n",
    "    'Hemochromatosis', 'Kidney Disease', 'Alzheimer\\'s Disease', 'Parkinson\\'s Disease', 'Balance Problems',\n",
    "    'Shingles', 'Osteoporosis', 'Age-related Macular Degeneration', 'Psoriasis', 'Gum (Periodontal) Disease', 'Dry Mouth'\n",
    "]\n",
    "\n",
    "# Split all multi-word phrases into individual words\n",
    "words_to_remove = set()\n",
    "for keyword in remove_keywords:\n",
    "    for word in re.findall(r'\\b\\w+\\b', keyword):\n",
    "        words_to_remove.add(word.lower())  # lowercased for case-insensitive match\n",
    "\n",
    "# Create regex pattern to match any of the words\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, words_to_remove)) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "# Remove individual words from each training text\n",
    "masked_train_texts = [pattern.sub('', text) for text in train_texts]\n",
    "\n",
    "# Normalize whitespace\n",
    "masked_train_texts = [re.sub(r'\\s+', ' ', text).strip() for text in masked_train_texts]\n",
    "\n",
    "print(f\"‚úÖ Keyword masking complete\")\n",
    "print(f\"   Removed {len(words_to_remove)} unique words\")\n",
    "print(f\"\\nüìù Example (before masking):\")\n",
    "print(train_texts[0][:200])\n",
    "print(f\"\\nüìù Example (after masking):\")\n",
    "print(masked_train_texts[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 5Ô∏è‚É£ Load BiomedBERT Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load BiomedBERT tokenizer and model\n",
    "MODEL_NAME = 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "\n",
    "print(f\"üì• Loading BiomedBERT model: {MODEL_NAME}\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_base = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model parameters: {sum(p.numel() for p in bert_base.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_classifier"
   },
   "outputs": [],
   "source": [
    "# Create BiomedBERT classifier (BERT base + classification head)\n",
    "class BiomedBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes=5):\n",
    "        super(BiomedBERTClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "bert_model = BiomedBERTClassifier(bert_base, num_classes=5).to(device)\n",
    "\n",
    "print(f\"‚úÖ Classifier created\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in bert_model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in bert_model.parameters() if p.requires_grad) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenization"
   },
   "source": [
    "## 6Ô∏è‚É£ Tokenization & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenize_train"
   },
   "outputs": [],
   "source": [
    "# Tokenize training data (EXACTLY from original notebook)\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "print(f\"üî§ Tokenizing training data (max_length={MAX_SEQ_LENGTH})...\")\n",
    "X_train = bert_tokenizer(\n",
    "    masked_train_texts,  # Use masked texts (keywords removed)\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "# Create training dataset and dataloader\n",
    "train_dataset = TensorDataset(\n",
    "    X_train['input_ids'].to(device),\n",
    "    X_train['attention_mask'].to(device),\n",
    "    y_train.to(device)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # batch_size=16 from notebook\n",
    "\n",
    "print(f\"‚úÖ Training data prepared\")\n",
    "print(f\"   Batches: {len(train_loader)}\")\n",
    "print(f\"   Batch size: 16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tokenize_test"
   },
   "outputs": [],
   "source": [
    "# Tokenize test data (NO masking for test data - evaluate on real data)\n",
    "print(f\"üî§ Tokenizing test data (max_length={MAX_SEQ_LENGTH})...\")\n",
    "X_test = bert_tokenizer(\n",
    "    test_texts,  # Use original test texts (no masking)\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = TensorDataset(\n",
    "    X_test['input_ids'].to(device),\n",
    "    X_test['attention_mask'].to(device),\n",
    "    y_test.to(device)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # batch_size=8 from notebook\n",
    "\n",
    "print(f\"‚úÖ Test data prepared\")\n",
    "print(f\"   Batches: {len(test_loader)}\")\n",
    "print(f\"   Batch size: 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 7Ô∏è‚É£ Training Configuration & Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "# Training configuration (EXACTLY from original notebook)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, bert_model.parameters()),\n",
    "    lr=0.00003  # 3e-5 from notebook\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10  # From notebook\n",
    "\n",
    "print(f\"‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Learning Rate: 3e-5 (0.00003)\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Loss Function: CrossEntropyLoss\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Training loop (EXACTLY from original notebook)\n",
    "print(f\"\\nüöÄ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    bert_model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_attention_mask, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = bert_model(batch_X, batch_attention_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average CE Loss: {avg_loss}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 8Ô∏è‚É£ Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(f\"üìä Evaluating model on test set...\\n\")\n",
    "\n",
    "bert_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_attention_mask, batch_y in test_loader:\n",
    "        outputs = bert_model(batch_X, batch_attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"üéØ Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"   Expected: ~99%\\n\")\n",
    "\n",
    "# Classification report\n",
    "label_names = [\n",
    "    'Neurological & Cognitive',\n",
    "    'Cancers',\n",
    "    'Cardiovascular',\n",
    "    'Metabolic & Endocrine',\n",
    "    'Other Age-Related & Immune'\n",
    "]\n",
    "print(f\"üìã Classification Report:\\n\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=label_names))\n",
    "\n",
    "# Confusion matrix\n",
    "print(f\"\\nüî¢ Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## 9Ô∏è‚É£ Save Model for Production Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save"
   },
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "import os\n",
    "\n",
    "output_dir = 'biomedbert_model'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the entire model (state dict + architecture)\n",
    "torch.save({\n",
    "    'model_state_dict': bert_model.state_dict(),\n",
    "    'model_config': {\n",
    "        'num_classes': 5,\n",
    "        'hidden_size': bert_base.config.hidden_size,\n",
    "        'model_name': MODEL_NAME\n",
    "    },\n",
    "    'label_mapping': focus_map,\n",
    "    'accuracy': accuracy\n",
    "}, f'{output_dir}/model.pt')\n",
    "\n",
    "# Save tokenizer\n",
    "bert_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save label mapping as JSON\n",
    "import json\n",
    "with open(f'{output_dir}/label_mapping.json', 'w') as f:\n",
    "    json.dump(focus_map, f, indent=2)\n",
    "\n",
    "# Save reverse mapping (for predictions)\n",
    "reverse_focus_map = {v: k for k, v in focus_map.items()}\n",
    "with open(f'{output_dir}/reverse_label_mapping.json', 'w') as f:\n",
    "    json.dump(reverse_focus_map, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to '{output_dir}/' directory\")\n",
    "print(f\"\\nüì¶ Files saved:\")\n",
    "print(f\"   - model.pt (PyTorch model weights + config)\")\n",
    "print(f\"   - tokenizer files (vocab, config, etc.)\")\n",
    "print(f\"   - label_mapping.json (focus group ‚Üí label)\")\n",
    "print(f\"   - reverse_label_mapping.json (label ‚Üí focus group)\")\n",
    "print(f\"\\nüì• Download the entire '{output_dir}/' folder and place it in your project's 'models/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üîü Download Model Files\n",
    "\n",
    "**Run the cell below to download the trained model as a ZIP file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_zip"
   },
   "outputs": [],
   "source": [
    "# Create ZIP file for easy download\n",
    "import shutil\n",
    "\n",
    "zip_filename = 'biomedbert_model'\n",
    "shutil.make_archive(zip_filename, 'zip', output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model packaged as {zip_filename}.zip\")\n",
    "print(f\"\\nüì• Downloading...\")\n",
    "\n",
    "# Download the ZIP file\n",
    "files.download(f'{zip_filename}.zip')\n",
    "\n",
    "print(f\"\\nüéâ Download complete!\")\n",
    "print(f\"\\nüìã Next Steps:\")\n",
    "print(f\"   1. Extract the ZIP file\")\n",
    "print(f\"   2. Place the 'biomedbert_model/' folder in your project's 'models/' directory\")\n",
    "print(f\"   3. Update your FastAPI app to load the model from 'models/biomedbert_model/'\")\n",
    "print(f\"   4. Test predictions with your API\")\n",
    "print(f\"\\n‚ú® Expected accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage"
   },
   "source": [
    "## üìù How to Use the Model in Your FastAPI App\n",
    "\n",
    "```python\n",
    "# In your FastAPI app (e.g., src/main.py or src/model.py)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "\n",
    "# Define the same model architecture\n",
    "class BiomedBERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes=5):\n",
    "        super(BiomedBERTClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load the model\n",
    "MODEL_DIR = 'models/biomedbert_model'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Load base BERT model\n",
    "checkpoint = torch.load(f'{MODEL_DIR}/model.pt', map_location=device)\n",
    "bert_base = AutoModel.from_pretrained(checkpoint['model_config']['model_name'])\n",
    "\n",
    "# Create classifier and load weights\n",
    "model = BiomedBERTClassifier(bert_base, num_classes=5).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Load label mapping\n",
    "with open(f'{MODEL_DIR}/reverse_label_mapping.json', 'r') as f:\n",
    "    label_to_focus_group = json.load(f)\n",
    "\n",
    "# Prediction function\n",
    "def predict(text: str):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        prediction = torch.argmax(outputs, dim=1).item()\n",
    "        probabilities = torch.softmax(outputs, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'focus_group': label_to_focus_group[str(prediction)],\n",
    "        'confidence': float(probabilities[prediction]),\n",
    "        'all_probabilities': {label_to_focus_group[str(i)]: float(prob) for i, prob in enumerate(probabilities)}\n",
    "    }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

